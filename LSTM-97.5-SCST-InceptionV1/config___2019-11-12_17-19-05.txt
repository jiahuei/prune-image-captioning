adam_epsilon = 0.01
add_grad_summaries = False
add_image_summaries = False
add_vars_summaries = False
attn_alignment_method = add
attn_context_layer = False
attn_keep_prob = 0.97
attn_map_loss_scale = 0.0
attn_num_heads = 1
attn_probability_fn = softmax
attn_size = 512
batch_size_eval = 61
batch_size_infer = 25
batch_size_train = 10
capture_profile = False
checkpoint_exclude_scopes = 
checkpoint_file_or_dir = /home/jiahuei/Documents/1_TF_files/prune/mscoco_v2/word_w256_LSTM_r512_xu_REG_0.0e+00_init_5.0_L1_wg_0.0_ann_sps_0.97_cnnFT_SCST_b7C1.0B2.0/run_01/model_compact-113287
checkpoint_path = /home/jiahuei/Documents/1_TF_files/prune/mscoco_v2/word_w256_LSTM_r512_xu_REG_0.0e+00_init_5.0_L1_wg_0.0_ann_sps_0.97_constLR_cnnFT/run_01
clip_gradient_norm = 0
cnn_fm_attention = Mixed_4f
cnn_fm_projection = independent
cnn_grad_multiplier = 1.0
cnn_input_augment = True
cnn_input_size = [224, 224]
cnn_name = inception_v1
dataset_dir = /home/jiahuei/Documents/3_Datasets/MSCOCO_captions
dataset_file_pattern = mscoco_{}_w5_s20_include_restval
freeze_scopes = Model/encoder/cnn
gpu = 0
infer_beam_size = 3
infer_length_penalty_weight = 0
infer_max_length = 30
infer_on_test = True
infer_set = test
initialiser = xavier_uniform
is_sparse = False
l2_decay = 1e-05
legacy = False
log_path = /home/jiahuei/Documents/1_TF_files/prune/mscoco_v2/word_w256_LSTM_r512_xu_REG_0.0e+00_init_5.0_L1_wg_0.0_ann_sps_0.97_cnnFT_SCST_b7C1.0B2.0/run_01_sparse___11-12_17-18
log_root = /home/jiahuei/Documents/1_TF_files/prune/
lr_end = 1e-05
lr_start = 0.001
max_epoch = 10
max_saves = 12
max_step = 113287
name = 
num_logs_per_epoch = 100
optimiser = adam
per_process_gpu_memory_fraction = None
radix_base = 256
rand_seed = 48964896
resume_training = False
rnn_init_method = project_hidden
rnn_keep_in = 0.89
rnn_keep_out = 0.89
rnn_layers = 1
rnn_name = LSTM
rnn_recurr_dropout = False
rnn_size = 512
rnn_word_size = 256
run = 1
save_path = /home/jiahuei/Documents/1_TF_files/prune/mscoco_v2/word_w256_LSTM_r512_xu_REG_0.0e+00_init_5.0_L1_wg_0.0_ann_sps_0.97_cnnFT_SCST_b7C1.0B2.0/run_01/model
save_unmasked_model = False
scst_beam_size = 7
scst_weight_bleu = [0.0, 0.0, 0.0, 2.0]
scst_weight_ciderD = 1.0
skip_att_loss_scale = 0.0
skip_att_switch = None
skip_att_temperature = 0.0
skip_att_threshold = 0.5
skip_time_step = None
split_sizes = {'infer': 5000}
supermask_grad_multiplier = 0.0
supermask_init_value = 5.0
supermask_loss_anneal = True
supermask_lr_start = 0.0
supermask_sparsity_loss_fn = L1
supermask_sparsity_target = 0.975
supermask_sparsity_weight = 0.0
supermask_train_strategy = sep_constant
supermask_type = None
token_type = word
train_mode = scst
vocab_size = 9962